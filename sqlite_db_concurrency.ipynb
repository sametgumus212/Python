{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sqlite_db_concurrency.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOE1V0oHHY5FWCu0moR92D1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sametgumus212/Python/blob/master/sqlite_db_concurrency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdtEbqCbatCu"
      },
      "source": [
        "# import os\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuxY_YClc1B1"
      },
      "source": [
        "pip install --upgrade jupyter_http_over_ws>=0.0.7 &&  jupyter serverextension enable --py jupyter_http_over_ws"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBirwI2Ic2vP"
      },
      "source": [
        "# jupyter notebook \\\n",
        "#   --NotebookApp.allow_origin='https://colab.research.google.com' \\\n",
        "#   --port=8888 \\\n",
        "#   --NotebookApp.port_retries=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hX2ja7RVvc9"
      },
      "source": [
        "!pip install bs4\n",
        "!pip install requests\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install urllib.request\n",
        "!pip install lxml\n",
        "!pip install html5lib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSxBh6OCoG43"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sqlite3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuM2Nl_Fo0us"
      },
      "source": [
        "conn=sqlite3.connect('All_news8.db')\n",
        "cursor=conn.cursor()\n",
        "cur=conn.cursor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFmkAypAxIRS",
        "outputId": "aebf0c20-79fe-4488-8611-daa17793f8b0"
      },
      "source": [
        "sqls=\"\"\" CREATE TABLE News(\n",
        "  Link TEXT,\n",
        "  Date TEXT,\n",
        "  Title TEXT,\n",
        "  Contain TEXT\n",
        ")\"\"\"\n",
        "cur.execute(sqls)\n",
        "print(\"created\")\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "o1tpkSBmb2Bi",
        "outputId": "1b8698fb-0b06-40be-d756-cd53a9b649a9"
      },
      "source": [
        "data=[]\n",
        "txt=\"\"\n",
        "a=\"\"\n",
        "b=\"\"\n",
        "c=\"\"\n",
        "s=0\n",
        "conn=sqlite3.connect('All_news.db')\n",
        "cur=conn.cursor()\n",
        "with open(\"docs/p_duma_bg_put.txt\", \"r\") as filestream:\n",
        "    # with open(\"answers.txt\", \"w\") as filestreamtwo:\n",
        "        for line in filestream:\n",
        "            if (len(line.split(\";\"))<5):          \n",
        "              cur.execute(\"INSERT INTO News VALUES(?,?,?,?)\", line.split(\";\"))\n",
        "              conn.commit()\n",
        "            else:\n",
        "              cl= line.split(\";\")\n",
        "              for i in range(len(cl)):\n",
        "                for i in range(3):\n",
        "\n",
        "                  a=cl[0]\n",
        "                  s+=1\n",
        "                  b=cl[1]\n",
        "                  s+=1\n",
        "                  c=cl[2]\n",
        "                  s+=1\n",
        "                \n",
        "                if(s>=3):\n",
        "                  txt+=cl[i]\n",
        "                  s+=1\n",
        "              query=\"\"\"INSERT INTO News  VALUES('{}','{}','{}','{}')\"\"\".format(a,b,c,txt) #(Link,Date,Title,Contain)\n",
        "              cur.execute(query)                \n",
        "              txt=\"\"\n",
        "              a=\"\"\n",
        "              b=\"\"\n",
        "              c=\"\"   \n",
        "conn.close()\n",
        "print(\"saved\")\n",
        "           "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e1a37d9f2aa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mconn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All_news.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"docs/p_duma_bg_put.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfilestream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sqlite3' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiJzKRd6RtaF"
      },
      "source": [
        "conn2=sqlite3.connect('All_news.db')\n",
        "crs=conn2.cursor()\n",
        "search_query=\"\"\"SELECT * from News\"\"\"\n",
        "crs.execute(search_query)\n",
        "records=crs.fetchall()\n",
        "for i in records:\n",
        "  print(i)\n",
        "crs.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oQlBY6owyIh"
      },
      "source": [
        "\n",
        "import time\n",
        "import multiprocessing\n",
        "import threading\n",
        "from multiprocessing import pool\n",
        "import concurrent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOHEX-Ajxilf"
      },
      "source": [
        "def file_to_db2(path):\n",
        "  data=[]\n",
        "  txt=\"\"\n",
        "  a=\"\"\n",
        "  b=\"\"\n",
        "  c=\"\"\n",
        "  s=0\n",
        "  h=0\n",
        "  conn=sqlite3.connect('All_news8.db')\n",
        "  cur=conn.cursor()\n",
        "  print(path)\n",
        "  print(\"sd\")\n",
        "  with open(path, \"r\") as filestream:\n",
        "          print(\"jjj\")\n",
        "      # with open(\"answers.txt\", \"w\") as filestreamtwo:\n",
        "          for line in filestream:\n",
        "              print(line,\"sdf\")\n",
        "              if (len(line.split(\";\"))<5):          \n",
        "                cur.execute(\"INSERT INTO News VALUES(?,?,?,?)\", line.split(\";\"))\n",
        "                conn.commit()\n",
        "              else:\n",
        "                cl= line.split(\";\")\n",
        "                for i in range(len(cl)):\n",
        "                  for i in range(3):\n",
        "\n",
        "                    a=cl[0]\n",
        "                    s+=1\n",
        "                    b=cl[1]\n",
        "                    s+=1\n",
        "                    c=cl[2]\n",
        "                    s+=1\n",
        "                  \n",
        "                  if(s>=3):\n",
        "                    txt+=cl[i]\n",
        "                    s+=1\n",
        "                query=\"\"\"INSERT INTO News  VALUES('{}','{}','{}','{}')\"\"\".format(a,b,c,txt) #(Link,Date,Title,Contain)\n",
        "                \n",
        "                \n",
        "                cur.execute(query)                \n",
        "                txt=\"\"\n",
        "                a=\"\"\n",
        "                b=\"\"\n",
        "                c=\"\"  \n",
        "              if(h>=4000):\n",
        "                  break\n",
        "              h+=1 \n",
        "  conn.close()\n",
        "  print(\"saved\")\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59oOM9RZzf2G",
        "outputId": "49008478-194e-40c6-de89-d92e064ccc79"
      },
      "source": [
        "t1=time.time()\n",
        "file_to_db()\n",
        "print(time.time()-t1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved\n",
            "350.8985381126404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmQ3Kou9w-cb",
        "outputId": "025ee9ef-9162-48ee-a744-238269c404f1"
      },
      "source": [
        "import time\n",
        "t7=time.time()\n",
        "zz=[]\n",
        "with concurrent.futures.ProcessPoolExecutor() as execu:\n",
        "  b_res=[execu.submit(file_to_db)]\n",
        "  for f in concurrent.futures.as_completed(b_res):\n",
        "     zz.append(f.result())\n",
        "    #  print(f.result())\n",
        "print(time.time()-t7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved\n",
            "352.241024017334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn8RAoWFRtqY"
      },
      "source": [
        "def readmultiple(files):\n",
        "  \n",
        "    # b_res=[execu.submit(creator,i.strip()) for i in a_p_links]\n",
        "  with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "    c_res=[execut.submit(file_to_db2,i) for i in files]\n",
        "    for f in concurrent.futures.as_completed(c_res):\n",
        "      print(f.result())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSgVAczqVLqB"
      },
      "source": [
        "filem=['docs/z_trud_bg_container_output.txt','docs/p_duma_bg_put.txt','docs/zp_trud_bg_container_output.txt']\n",
        "readmultiple(filem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ0r2_TwZM7q"
      },
      "source": [
        "def regularization():\n",
        "    # data=[]\n",
        "    txt=\"\"\n",
        "    a=\"\"\n",
        "    b=\"\"\n",
        "    c=\"\"\n",
        "    d=\"\"\n",
        "    s=0\n",
        "    newlink=\"\"\n",
        "    # cl= line.split(\";\")\n",
        "    content=\"\"\n",
        "   \n",
        "    with open(\"/content/birgun_contain_prostat_kanseri.txt\", \"r\") as filestream:        \n",
        "            for line in filestream:\n",
        "                s=0\n",
        "                cl= line.split(\";\") # \n",
        "                if (len(cl)<=5):      \n",
        "                  newlink=line    \n",
        "                 \n",
        "                else:                  \n",
        "                  for i in range(len(cl)):\n",
        "                    # for i in range(3):\n",
        "\n",
        "                    a=cl[0]\n",
        "                    s+=1\n",
        "                    b=cl[1]\n",
        "                    s+=1\n",
        "                    c=cl[2]\n",
        "                    s+=1\n",
        "                    d=cl[3]\n",
        "                    s+=1\n",
        "                  \n",
        "                  if (s>=4):\n",
        "                    txt+=cl[i]\n",
        "                    s+=1\n",
        "                  newlink=txt\n",
        "                with open(\"zuma_bg_put.txt\", \"a\") as f:\n",
        "                  f.write(newlink)\n",
        "                  newlink=\"\"\n",
        "                  txt=\"\"\n",
        "                  a=\"\"\n",
        "                  b=\"\"\n",
        "                  c=\"\"  \n",
        "                  d=\"\" \n",
        "\n",
        "    print(\"saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjsXI_YVMV6e",
        "outputId": "cb667f13-0906-4d7e-edc0-3f9051f001e8"
      },
      "source": [
        "regularization()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM1PirwekLQi"
      },
      "source": [
        "import json\n",
        "  \n",
        "  \n",
        "# the file to be converted\n",
        "filename = 'data.txt'\n",
        "  \n",
        "# resultant dictionary\n",
        "dict1 = {}\n",
        "  \n",
        "# fields in the sample file \n",
        "fields =['link', 'date', 'title', 'content']\n",
        "  \n",
        "with open(filename) as fh:\n",
        "      \n",
        "  \n",
        "      \n",
        "    # count variable for employee id creation\n",
        "    l = 1\n",
        "      \n",
        "    for line in fh:\n",
        "          \n",
        "        # reading line by line from the text file\n",
        "        description = list( line.strip().split(';', 4))\n",
        "          \n",
        "        # for output see below\n",
        "        print(description) \n",
        "          \n",
        "        # for automatic creation of id for each employee\n",
        "        sno ='news'+str(l)\n",
        "      \n",
        "        # loop variable\n",
        "        i = 0\n",
        "        # intermediate dictionary\n",
        "        dict2 = {}\n",
        "        while i<len(fields):\n",
        "              \n",
        "                # creating dictionary for each employee\n",
        "                dict2[fields[i]]= description[i]\n",
        "                i = i + 1\n",
        "                  \n",
        "        # appending the record of each employee to\n",
        "        # the main dictionary\n",
        "        dict1[sno]= dict2\n",
        "        l = l + 1\n",
        "  \n",
        "  \n",
        "# creating json file        \n",
        "out_file = open(\"test2.json\", \"w\")\n",
        "json.dump(dict1, out_file, indent = 4)\n",
        "out_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JWllnSuKfAY"
      },
      "source": [
        "\n",
        "# importing pandas library\n",
        "import pandas as pd\n",
        "  \n",
        "# reading given csv file \n",
        "# and creating dataframe\n",
        "websites = pd.read_csv(\"/content/zuma_bg_put.txt\",sep=';',header = None)\n",
        "  \n",
        "# adding column headings\n",
        "websites.columns = ['link', 'date', 'category','title','contain']\n",
        "  \n",
        "# store dataframe into csv file\n",
        "websites.to_csv('deneme3.csv', \n",
        "                index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "K3UvC-ezIgbM",
        "outputId": "70160a6f-eb02-4217-c2a3-ddcccb44547d"
      },
      "source": [
        "import pandas as pd\n",
        "  \n",
        "# readinag given csv file\n",
        "# and creating dataframe\n",
        "dataframe1 = pd.read_csv(\"/content/birgun_contain_prostat_kanseri.txt\",delimiter=';')\n",
        "  \n",
        "# storing this dataframe in a csv file\n",
        "dataframe1.to_csv('birgun_prostat.csv', \n",
        "                  index = None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-126de4cd0a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# readinag given csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# and creating dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataframe1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/birgun_contain_prostat_kanseri.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# storing this dataframe in a csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 5 fields in line 3, saw 6\n"
          ]
        }
      ]
    }
  ]
}